{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGnlRWvkY-2c"
   },
   "source": [
    "# Sentiment Analysis with BERT\n",
    "\n",
    "> TL;DR In this notebook, we fine-tune BERT for sentiment analysis. We perform the required dataset preparation using Pandas, and then we do the text preprocessing and build a Sentiment Classifier using the Transformers library by Hugging Face as well as Pytorch.\n",
    "\n",
    "Goal (official): To get a model that can classify text based on seniment, and - hopefully - performs well on neutral text, which can be problematic for some models.\n",
    "\n",
    "Goal (actual): To understand the entire pipeline better, become more skilled, and perhaps get a few ideas on how to perform some personal project ideas that I have. Also: For GPUs to go \"Brrrrrrrrrrrrrr\". (duh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiple resources were used in creating this notebook, but mostly, it's based on [this notebook](https://github.com/curiousily/Getting-Things-Done-with-Pytorch/blob/master/08.sentiment-analysis-with-bert.ipynb), made by [Venelin Valkov](https://www.youtube.com/@venelin_valkov). I don't like copy/pasting code that I don't understand, so I did not include some of the fancier sections from that notebook here. Other sources were used as well, including ChatGPT and other online sources.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "NJ6MhJYYBCwu",
    "outputId": "fc1bc767-990a-486d-bb10-2a80cea128b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 11 13:16:45 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.41                 Driver Version: 531.41       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 960M       WDDM | 00000000:02:00.0 Off |                  N/A |\n",
      "| N/A    0C    P0               N/A /  N/A|      0MiB /  4096MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Mon_Oct_24_19:40:05_Pacific_Daylight_Time_2022\n",
      "Cuda compilation tools, release 12.0, V12.0.76\n",
      "Build cuda_12.0.r12.0/compiler.31968024_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wmj22-TcZMef"
   },
   "source": [
    "## Setup\n",
    "\n",
    "We'll need to make sure Transformers is installed before we start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll also need to import the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w68CZpOwFoly",
    "outputId": "9c1a0321-1650-4224-cf9c-3c8dc8661ed3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict  \n",
    "# a dictionary-like object that initializes nonexistent keys, if called, to a default pre-chosen value\n",
    "\n",
    "from tqdm import tqdm\n",
    "# isn't it nice when you can personally sees the GPUs gp \"Brrrrr\"?\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the watermark extension for Jupyter notebooks here to view the versions of th most important libraries we'll be using. In case of a code error, version incompatibility is one of the usual suspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U watermark \n",
    "# -q is for \"quiet\". This means it won't output anything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "AJqoaFpVpoM8",
    "outputId": "88b5415f-9104-4937-c782-09c6025945c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.5\n",
      "IPython version      : 8.11.0\n",
      "\n",
      "numpy       : 1.23.5\n",
      "pandas      : 1.1.5\n",
      "torch       : 1.7.1\n",
      "transformers: 4.1.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reload the watermark extension for Jupyter\n",
    "%reload_ext watermark\n",
    "\n",
    "# use watermark to display the version info for the specified packages\n",
    "%watermark -v -p numpy,pandas,torch,transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufzPdoTtNikq"
   },
   "source": [
    "## Data Exploration\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. Download the [selected dataset](https://www.kaggle.com/datasets/jillanisofttech/amazon-product-reviews) from Kaggle using the kaggle API.\n",
    "2. Load our dataset using Pandas.\n",
    "3. View the basic structure of the dataset: `head()`\\\\`sample()`, `info()`, etc.\n",
    "4. Check for missing data, and for any needed changes. (here: heavy imbalance between sentiments was found)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by downloading the [selected dataset](https://www.kaggle.com/datasets/jillanisofttech/amazon-product-reviews) from Kaggle using the kaggle API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the instructions found on [Kaggle's website](https://www.kaggle.com/docs/api) to get our API token and authenticate it. We should know there was no issue with the authentication if we can run `import kaggle` successfully without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, seems like it works. This coding stuff must be real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's download our dataset. Using the convenient **\"Copy API command\"** option available on every Kaggle dataset page, we apply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading amazon-product-reviews.zip to C:\\Users\\ahmed\\Desktop\\Projects\\Technical\\Notebooks\\Getting Things Done with Pytorch\\Personal Projects\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/115M [00:00<?, ?B/s]\n",
      "  1%|          | 1.00M/115M [00:00<00:26, 4.46MB/s]\n",
      "  2%|1         | 2.00M/115M [00:00<00:19, 5.93MB/s]\n",
      "  3%|2         | 3.00M/115M [00:00<00:17, 6.56MB/s]\n",
      "  3%|3         | 4.00M/115M [00:00<00:15, 7.35MB/s]\n",
      "  4%|4         | 5.00M/115M [00:00<00:14, 7.87MB/s]\n",
      "  6%|6         | 7.00M/115M [00:00<00:11, 9.68MB/s]\n",
      "  8%|7         | 9.00M/115M [00:01<00:10, 10.9MB/s]\n",
      " 10%|9         | 11.0M/115M [00:01<00:09, 11.5MB/s]\n",
      " 11%|#1        | 13.0M/115M [00:01<00:09, 11.6MB/s]\n",
      " 13%|#3        | 15.0M/115M [00:01<00:08, 13.0MB/s]\n",
      " 15%|#4        | 17.0M/115M [00:01<00:07, 13.0MB/s]\n",
      " 17%|#6        | 19.0M/115M [00:02<00:11, 8.67MB/s]\n",
      " 18%|#8        | 21.0M/115M [00:02<00:12, 7.94MB/s]\n",
      " 20%|##        | 23.0M/115M [00:02<00:09, 9.71MB/s]\n",
      " 23%|##2       | 26.0M/115M [00:02<00:07, 13.0MB/s]\n",
      " 24%|##4       | 28.0M/115M [00:03<00:09, 9.72MB/s]\n",
      " 28%|##7       | 32.0M/115M [00:03<00:06, 13.1MB/s]\n",
      " 31%|###       | 35.0M/115M [00:03<00:06, 12.3MB/s]\n",
      " 33%|###3      | 38.0M/115M [00:03<00:05, 15.0MB/s]\n",
      " 35%|###4      | 40.0M/115M [00:03<00:04, 16.0MB/s]\n",
      " 37%|###6      | 42.0M/115M [00:03<00:05, 13.3MB/s]\n",
      " 38%|###8      | 44.0M/115M [00:04<00:05, 12.4MB/s]\n",
      " 40%|####      | 46.0M/115M [00:04<00:05, 12.3MB/s]\n",
      " 42%|####1     | 48.0M/115M [00:04<00:05, 13.8MB/s]\n",
      " 45%|####4     | 51.0M/115M [00:04<00:05, 11.9MB/s]\n",
      " 46%|####6     | 53.0M/115M [00:04<00:04, 12.9MB/s]\n",
      " 49%|####8     | 56.0M/115M [00:05<00:04, 14.2MB/s]\n",
      " 51%|#####     | 58.0M/115M [00:05<00:04, 13.5MB/s]\n",
      " 52%|#####2    | 60.0M/115M [00:05<00:06, 8.64MB/s]\n",
      " 54%|#####4    | 62.0M/115M [00:05<00:05, 9.86MB/s]\n",
      " 56%|#####5    | 64.0M/115M [00:05<00:04, 10.8MB/s]\n",
      " 58%|#####7    | 66.0M/115M [00:06<00:04, 11.3MB/s]\n",
      " 59%|#####9    | 68.0M/115M [00:06<00:05, 9.09MB/s]\n",
      " 61%|######1   | 70.0M/115M [00:06<00:06, 7.73MB/s]\n",
      " 62%|######2   | 71.0M/115M [00:07<00:06, 6.83MB/s]\n",
      " 63%|######2   | 72.0M/115M [00:07<00:07, 5.59MB/s]\n",
      " 64%|######3   | 73.0M/115M [00:07<00:11, 3.91MB/s]\n",
      " 65%|######4   | 74.0M/115M [00:08<00:14, 2.95MB/s]\n",
      " 66%|######5   | 75.0M/115M [00:09<00:24, 1.71MB/s]\n",
      " 66%|######6   | 76.0M/115M [00:10<00:26, 1.50MB/s]\n",
      " 67%|######7   | 77.0M/115M [00:11<00:25, 1.55MB/s]\n",
      " 68%|######8   | 78.0M/115M [00:11<00:22, 1.70MB/s]\n",
      " 69%|######8   | 79.0M/115M [00:12<00:19, 1.96MB/s]\n",
      " 70%|######9   | 80.0M/115M [00:12<00:15, 2.29MB/s]\n",
      " 71%|#######   | 81.0M/115M [00:12<00:12, 2.75MB/s]\n",
      " 72%|#######1  | 82.0M/115M [00:12<00:10, 3.30MB/s]\n",
      " 72%|#######2  | 83.0M/115M [00:13<00:08, 3.98MB/s]\n",
      " 73%|#######3  | 84.0M/115M [00:13<00:06, 4.77MB/s]\n",
      " 74%|#######4  | 85.0M/115M [00:13<00:05, 5.67MB/s]\n",
      " 76%|#######5  | 87.0M/115M [00:13<00:03, 7.58MB/s]\n",
      " 78%|#######7  | 89.0M/115M [00:13<00:02, 9.34MB/s]\n",
      " 79%|#######9  | 91.0M/115M [00:13<00:02, 9.58MB/s]\n",
      " 81%|########1 | 93.0M/115M [00:14<00:02, 8.84MB/s]\n",
      " 82%|########2 | 94.0M/115M [00:14<00:02, 8.56MB/s]\n",
      " 84%|########3 | 96.0M/115M [00:14<00:01, 10.7MB/s]\n",
      " 86%|########5 | 98.0M/115M [00:14<00:01, 11.2MB/s]\n",
      " 87%|########7 | 100M/115M [00:15<00:02, 6.93MB/s] \n",
      " 88%|########8 | 101M/115M [00:15<00:02, 5.83MB/s]\n",
      " 90%|########9 | 103M/115M [00:15<00:01, 6.22MB/s]\n",
      " 93%|#########2| 106M/115M [00:15<00:01, 8.10MB/s]\n",
      " 93%|#########3| 107M/115M [00:16<00:01, 6.75MB/s]\n",
      " 95%|#########5| 109M/115M [00:16<00:00, 6.36MB/s]\n",
      " 98%|#########7| 112M/115M [00:16<00:00, 9.22MB/s]\n",
      "100%|#########9| 114M/115M [00:16<00:00, 9.08MB/s]\n",
      "100%|##########| 115M/115M [00:16<00:00, 7.10MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d jillanisofttech/amazon-product-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code needs a `!` at the beginning, because what we installed above and are using to interact with the Kaggle API is actually a [CLI](https://en.wikipedia.org/wiki/Command-line_interface) tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above cell tells us that our zip file `amazon-product-reviews.zip` has been downloaded to the current directory.\n",
    "\n",
    "Now, let's unzip this zip file.\n",
    "\n",
    "I don't know/remember how to unzip using Python. Fortunately, Stack Overflow is a thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('amazon-product-reviews.zip', 'r') as zip_file:\n",
    "    zip_file.extractall('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove('amazon-product-reviews.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we now have a `Reviews.csv` file in our directory. Let's use Pandas to load it and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568454, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119015</th>\n",
       "      <td>119016</td>\n",
       "      <td>B003YF1188</td>\n",
       "      <td>A3MB83ALNB3O4Z</td>\n",
       "      <td>Ann</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350432000</td>\n",
       "      <td>I am a fan of Stonewall Kitchen!</td>\n",
       "      <td>I was so happy that Amazon carries this produc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120975</th>\n",
       "      <td>120976</td>\n",
       "      <td>B001EQ57KW</td>\n",
       "      <td>A2MM5OQCXV4BQ1</td>\n",
       "      <td>GAD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1338422400</td>\n",
       "      <td>Good for snacking, great for baking!</td>\n",
       "      <td>First, I love the fact that Go Raw processes t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306052</th>\n",
       "      <td>306053</td>\n",
       "      <td>B002R89LOE</td>\n",
       "      <td>AY1EF0GOH80EK</td>\n",
       "      <td>Natasha Stryker</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1276819200</td>\n",
       "      <td>Am I just crazy?</td>\n",
       "      <td>I feel like I am taking crazy pills when I rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485495</th>\n",
       "      <td>485496</td>\n",
       "      <td>B001RVFERK</td>\n",
       "      <td>AJFXMVJTGGHTY</td>\n",
       "      <td>Wade Osborne \"Wade Osborne\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1312761600</td>\n",
       "      <td>great chips</td>\n",
       "      <td>Pop Chips are the best chips I've had that are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490320</th>\n",
       "      <td>490321</td>\n",
       "      <td>B001E5DZJ8</td>\n",
       "      <td>A334K3EPD2H467</td>\n",
       "      <td>Sarah Norman</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1314662400</td>\n",
       "      <td>Tastes great</td>\n",
       "      <td>These taste great.  I add 1 or 2 cubes to the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                  ProfileName  \\\n",
       "119015  119016  B003YF1188  A3MB83ALNB3O4Z                          Ann   \n",
       "120975  120976  B001EQ57KW  A2MM5OQCXV4BQ1                          GAD   \n",
       "306052  306053  B002R89LOE   AY1EF0GOH80EK              Natasha Stryker   \n",
       "485495  485496  B001RVFERK   AJFXMVJTGGHTY  Wade Osborne \"Wade Osborne\"   \n",
       "490320  490321  B001E5DZJ8  A334K3EPD2H467                 Sarah Norman   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "119015                     0                       0      5  1350432000   \n",
       "120975                     0                       0      5  1338422400   \n",
       "306052                     7                       7      2  1276819200   \n",
       "485495                     0                       0      4  1312761600   \n",
       "490320                     0                       0      5  1314662400   \n",
       "\n",
       "                                     Summary  \\\n",
       "119015      I am a fan of Stonewall Kitchen!   \n",
       "120975  Good for snacking, great for baking!   \n",
       "306052                      Am I just crazy?   \n",
       "485495                           great chips   \n",
       "490320                          Tastes great   \n",
       "\n",
       "                                                     Text  \n",
       "119015  I was so happy that Amazon carries this produc...  \n",
       "120975  First, I love the fact that Go Raw processes t...  \n",
       "306052  I feel like I am taking crazy pills when I rea...  \n",
       "485495  Pop Chips are the best chips I've had that are...  \n",
       "490320  These taste great.  I add 1 or 2 cubes to the ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Reviews.csv')\n",
    "print(df.shape)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we really care about here are two columns: \"Text\" and \"Score\", as those are the ins and outs of the model.\n",
    "\n",
    "\"Summary\" and \"HelpfulnessDenominator\" can be useful to take a look at too, for a future project perhaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 568454 entries, 0 to 568453\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count   Dtype \n",
      "---  ------                  --------------   ----- \n",
      " 0   Id                      568454 non-null  int64 \n",
      " 1   ProductId               568454 non-null  object\n",
      " 2   UserId                  568454 non-null  object\n",
      " 3   ProfileName             568438 non-null  object\n",
      " 4   HelpfulnessNumerator    568454 non-null  int64 \n",
      " 5   HelpfulnessDenominator  568454 non-null  int64 \n",
      " 6   Score                   568454 non-null  int64 \n",
      " 7   Time                    568454 non-null  int64 \n",
      " 8   Summary                 568427 non-null  object\n",
      " 9   Text                    568454 non-null  object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 43.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have a fantastically complete dataset here! ~570K rows that are almost all complete, with a handful of missing values in the entire thing. Cool.\n",
    "\n",
    "The two columns we care about the most, `Text` and `Score` are complete as well. We get to live to fight (or clean data) another day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm still interested in filtering out those rows with null values in them in the \"Summary\" column, however. Maybe we could use that column later, so I'd like to drop these rows now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis='index', how='all', subset=['Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 568427 entries, 0 to 568453\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count   Dtype \n",
      "---  ------                  --------------   ----- \n",
      " 0   Id                      568427 non-null  int64 \n",
      " 1   ProductId               568427 non-null  object\n",
      " 2   UserId                  568427 non-null  object\n",
      " 3   ProfileName             568411 non-null  object\n",
      " 4   HelpfulnessNumerator    568427 non-null  int64 \n",
      " 5   HelpfulnessDenominator  568427 non-null  int64 \n",
      " 6   Score                   568427 non-null  int64 \n",
      " 7   Time                    568427 non-null  int64 \n",
      " 8   Summary                 568427 non-null  object\n",
      " 9   Text                    568427 non-null  object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 47.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a large datset such as this one, it's hard to avoid redundancy. People will repeat themselves, especially with shorter reviews. Let's see how many unique reviews we've got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393576"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.Text.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some 175K reviews are duplicates here, which is roughly the third of the entire dataset. Let's drop these now and save ourselves some precious GPU training hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset='Text', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393576"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's count the remaining reviews by score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    250716\n",
       "4     56042\n",
       "1     36275\n",
       "3     29752\n",
       "2     20791\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Score.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like most of the reviews are positive. Will this is certainly good for merchants, it's bad news for models that want to train hard to learn those better weights and biases. We'll deal with this imbalance in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now move to the next section, where we will get our data ready to be fed into Bert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufzPdoTtNikq"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. Create a function to turn the ratings which are currently numbers into 3 sentiment classes. \n",
    "2. Create a balanced dataset by making sure we have roughly equal neutral scores to +ve and -ve ones.\n",
    "3. Decide on a max. sequence length (in tokens) and trim longer texts to balance training cost vs. accuracy.\n",
    "4. Create our Pytorch Dataset class.\n",
    "5. Decide what batch size to use and create our Pytorch Dataloader class/function.\n",
    "6. Split the dataset into training, validation and test datasets, and create dataloaders for them.\n",
    "7. Examine one of our batches before moving on to the training section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by defining a `score2sentiment` function that will do as its name suggests. We'll define the scores 5 and 4 as being positive, 3 as being neutral, and 2 and 1 as being negative. You may want to stick with these definitions, or you might want to change them or add more classes. You would only need to keep in mind that any changes to this decision will carry out and require subsequent changes down the line. So we apply here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score2sentiment(score):\n",
    "    if score < 3 :\n",
    "        return 'negative'\n",
    "    elif score > 3 :\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this transformation to the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'] = df.Score.apply(score2sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now would be a good time to look at how well the data is distributed across sentiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    306758\n",
       "negative     57066\n",
       "neutral      29752\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't need all of this data for my current use case, so balancing the dataset by removing most of it is a luxury that I can afford:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_df = df[(df.Sentiment == 'positive')]\n",
    "negative_df = df[(df.Sentiment == 'negative')]\n",
    "neutral_df = df[(df.Sentiment == 'neutral')]\n",
    "\n",
    "balanced_df = pd.concat([positive_df.head(29_750), negative_df.head(29_750), neutral_df.head(29_750)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    29750\n",
       "negative    29750\n",
       "neutral     29750\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, going back to our to do list, we see that we will now be deciding on a maximum sequence length (in tokens), and that we need to trim longer texts to balance training cost vs. accuracy.\n",
    "\n",
    "Let's start by loading our tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now tokenize our dataset, but without setting any maximum length, in order to see how much of the dataset is longer than the maximum allowed sequence length we have in our model here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89250/89250 [01:23<00:00, 1064.05it/s]\n"
     ]
    }
   ],
   "source": [
    "tokens_lengths = []\n",
    "\n",
    "for review in tqdm(balanced_df.Text):\n",
    "    tokens = tokenizer.encode_plus(review,\n",
    "                                   add_special_tokens=True,\n",
    "                                   return_length=True)\n",
    "\n",
    "    tokens_lengths.append(tokens.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "great_tokens_lengths = [x for x in tokens_lengths if x > 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4980392156862745"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(great_tokens_lengths) / len(tokens_lengths)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, only 1.5% of the reviews are longer than 512 tokens, which is the limit for Bert. That means that only 1.5% of the dataset would be affected if we trim longer inputs. So we can safely decide on a max. len. of 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create our Pytorch Dataset class, followed by a small function that turns any dataset we pass to it into a DataLoader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "sentiment2number = {\"negative\" : 0, \"neutral\" : 1, \"positive\" : 2}\n",
    "\n",
    "class AmazonReviewDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, tokenizer,):\n",
    "        self.df = df\n",
    "        self.reviews = df.Text.to_numpy()\n",
    "        self.sentiments = df.Sentiment.to_numpy()\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    # We also need to define __len__() and __getitem__():\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df['Id'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        review = self.reviews[idx]\n",
    "        sentiment = self.sentiments[idx]\n",
    "        \n",
    "        sentiment = sentiment2number[sentiment]\n",
    "        sentiment = torch.tensor(sentiment, dtype=torch.long)\n",
    "        \n",
    "        tokens = self.tokenizer.encode_plus(review,\n",
    "                                             add_special_tokens=True,\n",
    "                                             max_length=512,\n",
    "                                             padding='max_length',\n",
    "                                             truncation=True,\n",
    "                                             return_token_type_ids=False,\n",
    "                                             return_tensors='pt')\n",
    "\n",
    "        return {\"review\": review, \"input_ids\": tokens['input_ids'].flatten(), \n",
    "                \"attention_mask\": tokens['attention_mask'].flatten(), \"sentiment\": sentiment}\n",
    "    \n",
    "# we must first transform sentiment to a number, then to type torch.long as that is required for classification\n",
    "# .flatten() turns our ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_dataloader(dataset, tokenizer):\n",
    "    processed_ds = AmazonReviewDataset(dataset, tokenizer)\n",
    "    \n",
    "    return DataLoader(processed_ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried a few numbers and settled with 16 as my batch size, but this will be experimental, and sometimes restricted based on the available hardware for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now shuffle the dataset, and then split it into training, validation and test datasets, and create dataloaders for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df = shuffle(balanced_df)\n",
    "\n",
    "train_df, val_df = train_test_split(shuffled_df, train_size=0.75)\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = create_dataloader(train_df, tokenizer)\n",
    "val_dl = create_dataloader(val_df, tokenizer)\n",
    "test_dl = create_dataloader(test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A6dlOptwqlhF"
   },
   "source": [
    "Finally, let's have a look at an example batch from our training data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['review', 'input_ids', 'attention_mask', 'sentiment'])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dl))\n",
    "\n",
    "print(batch.keys())\n",
    "print(batch['input_ids'].shape)\n",
    "print(batch['attention_mask'].shape)\n",
    "print(batch['sentiment'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H63Y-TjyRC7S"
   },
   "source": [
    "## Defining Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "440Nd31VTHER"
   },
   "source": [
    "Instead of using the Sentiment Analysis helper built for BERT which comes with the Transformers library, we'll use the basic `BertModel` and build our sentiment classifier on top of it.\n",
    "\n",
    "So in this section, we will:\n",
    "\n",
    "1. Load the base model (cased).\n",
    "2. run it on the sample text from previous section.\n",
    "3. Build a Sentiment Analysis wrapper around BERT using Pytorch.\n",
    "4. Show pre-trained BERT's ability to classify our sample text.\n",
    "5. Decide what loss function, optimizer, scheduler to use as well as the rest of the hyper-parameters.\n",
    "6. Create our training epoch function.\n",
    "7. Create our inference function.\n",
    "8. Finish with our training loop.\n",
    "9. View progress during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0P41FayISNRI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-cased', return_dict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFE7YSbFdY4t"
   },
   "source": [
    "And try to use it on the encoding of our sample text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s1aoFxbQSn15"
   },
   "outputs": [],
   "source": [
    "sample_input_ids = torch.reshape(batch[\"input_ids\"][0], (1, 512))\n",
    "sample_attention_mask = torch.reshape(batch[\"attention_mask\"][0], (1, 512))\n",
    "\n",
    "sample = {'input_ids':sample_input_ids, 'attention_mask': sample_attention_mask}\n",
    "raw, pooled = bert_model(**sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we had to reshape the tensors before feeding them into our model. To understand why, we need to consider the following:\n",
    "\n",
    "Shape of the input tensors **without** using `.flatten()`: `(batch_size, 1, sequence_length)` = `(16, 1, 512)`.\n",
    "\n",
    "Shape **with** using `.flatten()`: `(batch_size, sequence_length)` = `(16, 512)`.\n",
    "\n",
    "We can use `.flatten()` within `DataLoader`, presumably as it already has information about the batch size and preprocesses accordingly.\n",
    "\n",
    "We shouldn't use `.flatten()` for casual inference like we're doing now, as the model expects an input of a fixed length and shape (?) (Here: 512, since this is a BERT-model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4075,  0.2071,  0.1190,  ..., -0.4108,  0.0408, -0.0849],\n",
       "         [ 0.2758, -0.6349,  0.2261,  ..., -0.2547,  0.1820,  0.0482],\n",
       "         [ 0.5432, -0.6583,  0.1456,  ...,  0.6100,  0.6814, -0.2100],\n",
       "         ...,\n",
       "         [ 0.1340,  0.2772, -0.2694,  ...,  0.1435,  0.1378,  0.2156],\n",
       "         [ 0.1539,  0.2833, -0.3036,  ...,  0.1528,  0.1631,  0.2584],\n",
       "         [ 0.0829,  0.2475, -0.2725,  ...,  0.1568,  0.2055,  0.2240]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mLLu8zmqbaHV"
   },
   "source": [
    "The raw output of the model, `raw`, is a sequence of hidden states of the last layer of the model. Obtaining the pooled output `pooled` is done by applying pooling on the last hidden state `raw`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wTKi8-rTd_j4"
   },
   "source": [
    "You can think of `pooled` as a summary of the content, according to Bert. Let's look at the shape of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512, 768]), torch.Size([1, 768]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.shape, pooled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now move on to building our Sentiment Analysis wrapper around BERT using Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_mRflxPl32F"
   },
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased', return_dict=False)\n",
    "        self.drop = nn.Dropout(p=0.25)\n",
    "        self.output_layer = nn.Linear(768, num_classes)\n",
    "        # self.softmax = nn.Softmax(dim=1) # Why 1? to apply it among classes and not batches. Tell me more.\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        raw, pooled = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = self.drop(pooled)\n",
    "        fine_tuned = self.output_layer(pooled)\n",
    "        # classified = self.softmax(fine_tuned)\n",
    "        \n",
    "        return fine_tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UJg8m3NQJahc"
   },
   "source": [
    "We use a dropout layer for some regularization and a fully-connected layer for our output. **Note that we're returning the raw output of the last layer since that is required for the cross-entropy loss function in PyTorch to work.**\n",
    "\n",
    "Let's now initialize the model, move it to the GPU and show pre-trained Bert's ability to classify our sample text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(df.Sentiment.unique())\n",
    "\n",
    "classifier = SentimentClassifier(num_classes)\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i0yQnuSFsjDp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEWARE  I recieved my box of 25 assorted Nonnie's Biscotti from Amazon . Every single biscotti was broken in 1-3 pieces. I am disgusted completely as these were to be favors of a wedding . I will never buy from this company again.  BEWARE\n",
      "tensor(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number2sentiment = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "sample = {'input_ids':sample_input_ids.to(device), 'attention_mask': sample_attention_mask.to(device)}\n",
    "\n",
    "pooled = classifier(**sample)\n",
    "probs = F.softmax(pooled, dim=1)\n",
    "\n",
    "print(batch['review'][0])\n",
    "print(batch['sentiment'][0])\n",
    "\n",
    "result = number2sentiment[torch.argmax(probs).item()]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.detach_()\n",
    "# del output\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will now decide what loss function, optimizer and scheduler to use, as well as the rest of our hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "76g7FV85H-T8"
   },
   "source": [
    "We need to define:\n",
    "\n",
    "1. Number of epochs.\n",
    "2. Learning rate.\n",
    "3. Our optimizer.\n",
    "4. Our scheduler.\n",
    "5. Our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5v-ArJ2fCCcU"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "lr = 5e-5\n",
    "\n",
    "num_steps = EPOCHS * len(train_dl)\n",
    "\n",
    "optimizer = optim.AdamW(classifier.parameters(), lr=lr)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer=optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=num_steps)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A8522g7JIu5J"
   },
   "source": [
    "How come those values?\n",
    "\n",
    "Well, the number of epochs and the learning rate are among the recommended values in the original [Bert paper](https://arxiv.org/abs/1810.04805).\n",
    "\n",
    "The loss function is Cross Entropy, which is commonly used for Sentiment Analysis and is suitable because it always outputs values that add up to 1, which is exactly what all probabilities add up to in any scenario. That means we can always use it for problems with a defined number of classes.\n",
    "\n",
    "The optimizer and scheduler are the ones used in the [original version](https://github.com/curiousily/Getting-Things-Done-with-Pytorch/blob/master/08.sentiment-analysis-with-bert.ipynb) of this notebook, by [Venelin Valkov](https://www.youtube.com/@venelin_valkov)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g9xikRdtRN1N"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally ready to create our training epoch function, followed by our inference function, and our training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bzl9UhuNx1_Q"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, loss_fn, optimizer, scheduler, device):\n",
    "    # 1. Set the model in training mode\n",
    "    model = model.train()\n",
    "    \n",
    "    # 2. Initialize the variables for tracking loss and correct predictions\n",
    "    loss_history = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    # 3. Iterate over the batches in the data loader\n",
    "    for batch in tqdm(dataloader):\n",
    "        \n",
    "        # a. Move the input and target tensors to the specified device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        targets = batch['sentiment'].to(device)\n",
    "        \n",
    "        # b. Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # only returns the processed 'pooled', as we defined in our S.C. class\n",
    "        \n",
    "        values, predictions = torch.max(outputs, dim=1) \n",
    "        # specifying the dim. changes the returns of torch.max()\n",
    "        \n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        # c. Update the loss and correct predictions variables\n",
    "        loss_history.append(loss.item())\n",
    "        correct_predictions += torch.sum(predictions == targets)\n",
    "        # torch compares values for numbers, regardless of dtype\n",
    "        \n",
    "        # d. Backward pass\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # 4. Compute the accuracy and average loss\n",
    "    accuracy = correct_predictions.double()/(len(dataloader) * 16)\n",
    "    average_loss = np.mean(loss_history)\n",
    "        \n",
    "    return accuracy, average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E4PniYIte0fr"
   },
   "source": [
    "The training loop should look familiar, except for two things: The scheduler gets called every time a batch is fed to the model, an we're avoiding exploding gradients by clipping the gradients of the model using [clip_grad_norm_](https://pytorch.org/docs/stable/nn.html#clip-grad-norm).\n",
    "\n",
    "Let's now write the evaluation loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXeRorVGIKre"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, loss_fn, device):\n",
    "    # 1. Set the model in evaluation mode\n",
    "    model = model.eval()\n",
    "    \n",
    "    # 2. Initialize the variables for tracking loss and correct predictions\n",
    "    loss_history = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 3. Iterate over the batches in the data loader\n",
    "        for batch in tqdm(dataloader):\n",
    "\n",
    "            # a. Move the input and target tensors to the specified device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            targets = batch['sentiment'].to(device)\n",
    "\n",
    "            # b. Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)        \n",
    "            values, predictions = torch.max(outputs, dim=1)        \n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # c. Update the loss and correct predictions variables\n",
    "            loss_history.append(loss.item())\n",
    "            correct_predictions += torch.sum(predictions == targets)\n",
    "\n",
    "        # 4. Compute the accuracy and average loss\n",
    "        accuracy = correct_predictions.double()/(len(dataloader) * 16)\n",
    "        average_loss = np.mean(loss_history)\n",
    "\n",
    "    return accuracy, average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_rdSDBHhhCh"
   },
   "source": [
    "Using those two, we can write our training loop. We'll also store the training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "1zhHoFNsxufs",
    "outputId": "2f11710a-700e-4933-b57e-5d50e5ed1f78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 3.\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4184/4184 [39:46<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training dataset: 0.7467286089866156\n",
      "Loss on training dataset: 0.5931135959189606\n",
      "-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 698/698 [02:30<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation dataset: 0.7821454154727794\n",
      "Loss on validation dataset: 0.53433882863525\n",
      "-----\n",
      "Epoch 2 out of 3.\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4184/4184 [39:54<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training dataset: 0.833144120458891\n",
      "Loss on training dataset: 0.4173259054600879\n",
      "-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 698/698 [02:30<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation dataset: 0.7933381088825214\n",
      "Loss on validation dataset: 0.4978796033653028\n",
      "-----\n",
      "Epoch 3 out of 3.\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4184/4184 [39:53<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training dataset: 0.9087595602294455\n",
      "Loss on training dataset: 0.25616456334308646\n",
      "-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 698/698 [02:30<00:00,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation dataset: 0.7905623209169054\n",
      "Loss on validation dataset: 0.6446215840600922\n",
      "-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.a) initialize a dict. with default values that are lists\n",
    "history = defaultdict(list)\n",
    "\n",
    "# 1.b) store the highest val. acc. seen so far\n",
    "best_accuracy = 0\n",
    "\n",
    "# 2. loop through epochs\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    # a. Print the current epoch number\n",
    "    print(f\"Epoch {epoch + 1} out of {EPOCHS}.\")\n",
    "    print('-' * 10)\n",
    "\n",
    "    # b. Train the model and print output\n",
    "    train_acc, train_loss = train_one_epoch(classifier, train_dl, loss_fn, optimizer, scheduler, device)\n",
    "    print(f\"Accuracy on training dataset: {train_acc}\")\n",
    "    print(f\"Loss on training dataset: {train_loss}\")\n",
    "    print('-' * 5)\n",
    "\n",
    "    # c. Evaluate on the validation dataset and print output\n",
    "    val_acc, val_loss = evaluate_model(classifier, val_dl, loss_fn, device)\n",
    "    print(f\"Accuracy on validation dataset: {val_acc}\")\n",
    "    print(f\"Loss on validation dataset: {val_loss}\")\n",
    "    print('-' * 5)\n",
    "\n",
    "    # d. Document the training and validation accuracy and loss in history\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    # If current val_acc > best_accuracy, model state is saved to file\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(classifier.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3HZb3NWFtFf"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Here, we will:\n",
    "1. Write a function to test how well the model generalizes, by using the test dataset, which is data it didn't train on.\n",
    "2. Evaluate the model's performance using classification report.\n",
    "3. Try it on raw input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgR6MuNS8jr_"
   },
   "outputs": [],
   "source": [
    "def get_predictions_from_dl(model, dataloader):\n",
    "    \n",
    "    # 1. Set the model to evaluation mode\n",
    "    model = model.eval()\n",
    "    \n",
    "    # 2. Initialize the lists for ins, true_outs, preds and confidence \n",
    "    reviews = []\n",
    "    predictions = []\n",
    "    confidence_levels = []\n",
    "    true_sentiments = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 3. Iterate over the batches in the data loader\n",
    "        for batch in tqdm(dataloader):\n",
    "            # a. Move the model input tensors to the specified device,\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # keeping the ins & outs of the dataset unmoved\n",
    "            texts = batch['review']\n",
    "            # using a different variable name to not overwrite the list\n",
    "            sentiments = batch['sentiment']\n",
    "\n",
    "            # b. Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            vals, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            # c. Get confidence levels with softmax \n",
    "            confidence_lvls = F.softmax(outputs, dim=1)\n",
    "\n",
    "            # d. Update the lists\n",
    "            reviews.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            confidence_levels.extend(confidence_lvls)\n",
    "            true_sentiments.extend(sentiments)\n",
    "\n",
    "    # 4. Stack, move back to cpu and return everything\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    confidence_levels = torch.stack(confidence_levels).cpu()\n",
    "    true_sentiments = torch.stack(true_sentiments).cpu()\n",
    "    \n",
    "    return reviews, predictions, confidence_levels, true_sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkbnBTI7kd_y"
   },
   "source": [
    "This is similar to the evaluation function, except ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 698/698 [02:19<00:00,  4.99it/s]\n"
     ]
    }
   ],
   "source": [
    "test_reviews, test_predictions, test_confidence_lvls, test_sentiments = get_predictions_from_dl(classifier, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVwoVij2lC7F"
   },
   "source": [
    "Let's have a look at the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "L8a9_8-ND3Is",
    "outputId": "9b2c48cc-b62e-41f3-dba5-af90457a37de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.79      0.78      0.78      3706\\n           1       0.70      0.72      0.71      3785\\n           2       0.89      0.88      0.88      3666\\n\\n    accuracy                           0.79     11157\\n   macro avg       0.79      0.79      0.79     11157\\nweighted avg       0.79      0.79      0.79     11157\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(test_sentiments, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iANBiY3sLo-K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The price is good, size is perfect...but my dog doesn't like them at all. Would not buy again sorry. Not the products fault\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'negative': tensor(0.7084),\n",
       " 'neutral': tensor(0.2869),\n",
       " 'positive': tensor(0.0047)}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 57\n",
    "\n",
    "review = test_reviews[idx]\n",
    "confidence_lvls = test_confidence_lvls[idx]\n",
    "\n",
    "print(review)\n",
    "conf_lvls = {number2sentiment[idx] : confidence_lvls[idx] for idx in range(3)}\n",
    "conf_lvls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WL5pDmvFyaU"
   },
   "source": [
    "### Predicting on Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = r\"\"\"Very easy game. Explained in a minute. Anyone can play it. Very fast laps. The limit could be another two or three centimeters shorter,\n",
    "then it will “click” faster! I did it this way. It can be packed up to a small size and is almost indestructible.\n",
    "But it can easily lead to frustration if you play with too much ambition. You also have to be able to lose!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode_plus(Text, add_special_tokens=True, max_length=512,\n",
    "                                 padding='max_length', truncation=True,\n",
    "                                 return_token_type_ids=False, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': 0.001904321019537747, 'neutral': 0.023873839527368546, 'positive': 0.97422194480896}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "pooled = classifier(input_ids, attention_mask)\n",
    "\n",
    "confidence = F.softmax(pooled, dim=1).tolist()[0]\n",
    "confidence = {number2sentiment[idx] : confidence[idx] for idx in range(3)}\n",
    "\n",
    "val, prediction = torch.max(pooled, dim=1)\n",
    "prediction = number2sentiment[prediction.item()]\n",
    "\n",
    "print(confidence)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PVhwzq7bpPRl"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wf39tauBa2V2"
   },
   "source": [
    "## References\n",
    "\n",
    "- [Huggingface Transformers](https://huggingface.co/transformers/)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "08.sentiment-analysis-with-bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
