{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGnlRWvkY-2c"
   },
   "source": [
    "# Sentiment Analysis with BERT\n",
    "\n",
    "> TL;DR In this notebook, we fine-tune BERT for sentiment analysis. We perform the required text preprocessing (special tokens, padding, and attention masks) and build a Sentiment Classifier using the Transformers library by Hugging Face as well as Pytorch.\n",
    "\n",
    "Goal (official): To get a model that can classify text based on seniment, and hopefully it performs well on neutral text, which can be problematic for some models.\n",
    "\n",
    "Goal (actual): To understand the entire pipeline better, become more skilled, and perhaps get a few ideas on how to perform some personal project ideas that I have. Also, For GPUs to go \"Brrrrrrrrrrrrrr\". (duh)\n",
    "\n",
    "- Evaluate the model on test data\n",
    "- Predict sentiment on raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "NJ6MhJYYBCwu",
    "outputId": "fc1bc767-990a-486d-bb10-2a80cea128b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr  6 01:14:10 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A10          On   | 00000000:06:00.0 Off |                    0 |\n",
      "|  0%   31C    P8    20W / 150W |      0MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_Mar__8_18:18:20_PST_2022\n",
      "Cuda compilation tools, release 11.6, V11.6.124\n",
      "Build cuda_11.6.r11.6/compiler.31057947_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wmj22-TcZMef"
   },
   "source": [
    "## Setup\n",
    "\n",
    "We'll need to import the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w68CZpOwFoly",
    "outputId": "9c1a0321-1650-4224-cf9c-3c8dc8661ed3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict  \n",
    "# a dictionary-like object that initializes nonexistent keys, if called, to a default pre-chosen value\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the watermark extension for Jupyter notebooks here to view the versions of th most important libraries we'll be using. In case of a code error, version incompatibility is one of the usual suspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U watermark \n",
    "# -q means \"quiet\". This means it won't output anything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "AJqoaFpVpoM8",
    "outputId": "88b5415f-9104-4937-c782-09c6025945c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.10\n",
      "IPython version      : 7.13.0\n",
      "\n",
      "numpy       : 1.23.4\n",
      "pandas      : 1.5.1\n",
      "torch       : 1.12.1\n",
      "transformers: 4.27.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reload the watermark extension for Jupyter\n",
    "%reload_ext watermark\n",
    "\n",
    "# use watermark to display the version info for the specified packages\n",
    "%watermark -v -p numpy,pandas,torch,transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufzPdoTtNikq"
   },
   "source": [
    "## Data Exploration\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "0. Download the [selected] dataset from Kaggle using the kaggle API.\n",
    "1. Load our dataset using Pandas.\n",
    "2. View the basic structure of the dataset: `head()`\\\\`sample()`, `shape`, `info()`.\n",
    "3. Check for missing data or ...\n",
    "4. Check if the dataset already has clear sentiment classes defined, if not, create a way (here: a function) to establish those classes and decide on their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the instructions found on [Kaggle's website](https://www.kaggle.com/docs/api) to get our API token and authenticate it. We should know there was no issue with the authentication if we can run `import kaggle` successfully without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, seems like it works. This coding stuff must be real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's download our dataset. Using the convenient **\"Copy API command\"** option available on every Kaggle dataset page, we apply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets download -d jillanisofttech/amazon-product-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code needs a `!` at the beginning, because what we installed above and are using to interact with the Kaggle API is actually a [CLI](https://en.wikipedia.org/wiki/Command-line_interface) tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above cell tells us that our zip file `amazon-product-reviews.zip` has been downloaded to the current directory.\n",
    "\n",
    "Now, let's unzip this zip file.\n",
    "\n",
    "Problem: I don't know/remember how to unzip using Python.\n",
    "\n",
    "Solution: Stack Overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('amazon-product-reviews.zip', 'r') as zip_file:\n",
    "#     zip_file.extractall('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.remove('amazon-product-reviews.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we now have a `Reviews.csv` file in our directory. Let's use Pandas to load - and take a look at - this baby:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568454, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119015</th>\n",
       "      <td>119016</td>\n",
       "      <td>B003YF1188</td>\n",
       "      <td>A3MB83ALNB3O4Z</td>\n",
       "      <td>Ann</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350432000</td>\n",
       "      <td>I am a fan of Stonewall Kitchen!</td>\n",
       "      <td>I was so happy that Amazon carries this produc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120975</th>\n",
       "      <td>120976</td>\n",
       "      <td>B001EQ57KW</td>\n",
       "      <td>A2MM5OQCXV4BQ1</td>\n",
       "      <td>GAD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1338422400</td>\n",
       "      <td>Good for snacking, great for baking!</td>\n",
       "      <td>First, I love the fact that Go Raw processes t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306052</th>\n",
       "      <td>306053</td>\n",
       "      <td>B002R89LOE</td>\n",
       "      <td>AY1EF0GOH80EK</td>\n",
       "      <td>Natasha Stryker</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1276819200</td>\n",
       "      <td>Am I just crazy?</td>\n",
       "      <td>I feel like I am taking crazy pills when I rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485495</th>\n",
       "      <td>485496</td>\n",
       "      <td>B001RVFERK</td>\n",
       "      <td>AJFXMVJTGGHTY</td>\n",
       "      <td>Wade Osborne \"Wade Osborne\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1312761600</td>\n",
       "      <td>great chips</td>\n",
       "      <td>Pop Chips are the best chips I've had that are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490320</th>\n",
       "      <td>490321</td>\n",
       "      <td>B001E5DZJ8</td>\n",
       "      <td>A334K3EPD2H467</td>\n",
       "      <td>Sarah Norman</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1314662400</td>\n",
       "      <td>Tastes great</td>\n",
       "      <td>These taste great.  I add 1 or 2 cubes to the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                  ProfileName  \\\n",
       "119015  119016  B003YF1188  A3MB83ALNB3O4Z                          Ann   \n",
       "120975  120976  B001EQ57KW  A2MM5OQCXV4BQ1                          GAD   \n",
       "306052  306053  B002R89LOE   AY1EF0GOH80EK              Natasha Stryker   \n",
       "485495  485496  B001RVFERK   AJFXMVJTGGHTY  Wade Osborne \"Wade Osborne\"   \n",
       "490320  490321  B001E5DZJ8  A334K3EPD2H467                 Sarah Norman   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "119015                     0                       0      5  1350432000   \n",
       "120975                     0                       0      5  1338422400   \n",
       "306052                     7                       7      2  1276819200   \n",
       "485495                     0                       0      4  1312761600   \n",
       "490320                     0                       0      5  1314662400   \n",
       "\n",
       "                                     Summary  \\\n",
       "119015      I am a fan of Stonewall Kitchen!   \n",
       "120975  Good for snacking, great for baking!   \n",
       "306052                      Am I just crazy?   \n",
       "485495                           great chips   \n",
       "490320                          Tastes great   \n",
       "\n",
       "                                                     Text  \n",
       "119015  I was so happy that Amazon carries this produc...  \n",
       "120975  First, I love the fact that Go Raw processes t...  \n",
       "306052  I feel like I am taking crazy pills when I rea...  \n",
       "485495  Pop Chips are the best chips I've had that are...  \n",
       "490320  These taste great.  I add 1 or 2 cubes to the ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Reviews.csv')\n",
    "print(df.shape)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we really care about here are two columns: \"Text\" and \"Score\".\n",
    "\n",
    "\"Summary\" and \"HelpfulnessDenominator\" can be useful to take a look at too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 568454 entries, 0 to 568453\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count   Dtype \n",
      "---  ------                  --------------   ----- \n",
      " 0   Id                      568454 non-null  int64 \n",
      " 1   ProductId               568454 non-null  object\n",
      " 2   UserId                  568454 non-null  object\n",
      " 3   ProfileName             568438 non-null  object\n",
      " 4   HelpfulnessNumerator    568454 non-null  int64 \n",
      " 5   HelpfulnessDenominator  568454 non-null  int64 \n",
      " 6   Score                   568454 non-null  int64 \n",
      " 7   Time                    568454 non-null  int64 \n",
      " 8   Summary                 568427 non-null  object\n",
      " 9   Text                    568454 non-null  object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 43.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a fantastically complete dataset here! ~570K rows that are almost all complete, with a handful of missing values in the entire thing. Cool.\n",
    "\n",
    "The two columns we care about the most, `Text` and `Score` are complete as well. We get to live to fight (or clean data) another day!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " However, i'm interested in filtering out those rows with null values in them in the \"Summary\" column. Maybe we could that column later, so I'd like to drop these rows now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis='index', how='all', subset=['Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 568427 entries, 0 to 568453\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count   Dtype \n",
      "---  ------                  --------------   ----- \n",
      " 0   Id                      568427 non-null  int64 \n",
      " 1   ProductId               568427 non-null  object\n",
      " 2   UserId                  568427 non-null  object\n",
      " 3   ProfileName             568411 non-null  object\n",
      " 4   HelpfulnessNumerator    568427 non-null  int64 \n",
      " 5   HelpfulnessDenominator  568427 non-null  int64 \n",
      " 6   Score                   568427 non-null  int64 \n",
      " 7   Time                    568427 non-null  int64 \n",
      " 8   Summary                 568427 non-null  object\n",
      " 9   Text                    568427 non-null  object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 47.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393576"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.Text.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset='Text', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393576"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    250716\n",
       "4     56042\n",
       "1     36275\n",
       "3     29752\n",
       "2     20791\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Score.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufzPdoTtNikq"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. Create a function to turn the ratings which are currently numbers into 3 sentiment classes. \n",
    "2. Create a balanced dataset by making sure we have roughly equal neutral scores to +ve and -ve ones.\n",
    "3. Decide on a max. sequence length (in tokens) and trim longer texts to balance training cost vs. accuracy.\n",
    "4. Create our Pytorch Dataset class.\n",
    "5. Create our Pytorch Dataloaders class/function (which is better?)\n",
    "6. Split the dataset into training, validation and test datasets, and create dataloaders for them.\n",
    "7. Decide what batch size to use, and examine one of our batches before moving on to the training section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score2sentiment(score):\n",
    "    if score < 3 :\n",
    "        return 'negative'\n",
    "    elif score > 3 :\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'] = df.Score.apply(score2sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    306758\n",
       "negative     57066\n",
       "neutral      29752\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_df = df[(df.Sentiment == 'positive')]\n",
    "negative_df = df[(df.Sentiment == 'negative')]\n",
    "neutral_df = df[(df.Sentiment == 'neutral')]\n",
    "\n",
    "balanced_df = pd.concat([positive_df.head(29_750), negative_df.head(29_750), neutral_df.head(29_750)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    29750\n",
       "negative    29750\n",
       "neutral     29750\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89250/89250 [01:23<00:00, 1064.05it/s]\n"
     ]
    }
   ],
   "source": [
    "tokens_lengths = []\n",
    "\n",
    "for review in tqdm(balanced_df.Text):\n",
    "    tokens = tokenizer.encode_plus(review,\n",
    "                                   add_special_tokens=True,\n",
    "                                   return_length=True)\n",
    "\n",
    "    tokens_lengths.append(tokens.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "great_tokens_lengths = [x for x in tokens_lengths if x > 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4980392156862745"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(great_tokens_lengths) / len(tokens_lengths)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max len = 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at counts: Let's create a more balanced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    250716\n",
       "4     56042\n",
       "1     36275\n",
       "3     29752\n",
       "2     20791\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(df.Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "sentiment2number = {\"negative\" : 0, \"neutral\" : 1, \"positive\" : 2}\n",
    "\n",
    "class AmazonReviewDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, tokenizer,):\n",
    "        self.df = df\n",
    "        self.reviews = df.Text.to_numpy()\n",
    "        self.sentiments = df.Sentiment.to_numpy()\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    # We also need to define __len__() and __getitem__():\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df['Id'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        review = self.reviews[idx]\n",
    "        sentiment = self.sentiments[idx]\n",
    "        \n",
    "        sentiment = sentiment2number[sentiment]\n",
    "        sentiment = torch.tensor(sentiment, dtype=torch.long)\n",
    "        \n",
    "        tokens = self.tokenizer.encode_plus(review,\n",
    "                                             add_special_tokens=True,\n",
    "                                             max_length=512,\n",
    "                                             padding='max_length',\n",
    "                                             truncation=True,\n",
    "                                             return_token_type_ids=False,\n",
    "                                             return_tensors='pt')\n",
    "\n",
    "        return {\"review\": review, \"input_ids\": tokens['input_ids'].flatten(), \n",
    "                \"attention_mask\": tokens['attention_mask'].flatten(), \"sentiment\": sentiment}\n",
    "    \n",
    "# we must first transform sentiment to a number, then to type torch.long as that is required for classification\n",
    "# .flatten() turns our ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_dataloader(dataset, tokenizer):\n",
    "    processed_ds = AmazonReviewDataset(dataset, tokenizer)\n",
    "    \n",
    "    return DataLoader(processed_ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df = shuffle(balanced_df)\n",
    "\n",
    "train_df, val_df = train_test_split(shuffled_df, train_size=0.75)\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = create_dataloader(train_df, tokenizer)\n",
    "val_dl = create_dataloader(val_df, tokenizer)\n",
    "test_dl = create_dataloader(test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A6dlOptwqlhF"
   },
   "source": [
    "Let's have a look at an example batch from our training data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['review', 'input_ids', 'attention_mask', 'sentiment'])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dl))\n",
    "\n",
    "print(batch.keys())\n",
    "print(batch['input_ids'].shape)\n",
    "print(batch['attention_mask'].shape)\n",
    "print(batch['sentiment'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H63Y-TjyRC7S"
   },
   "source": [
    "## Defining Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "440Nd31VTHER"
   },
   "source": [
    "Instead of using the Sentiment Analysis helper built for BERT which comes with the Transformers library, we'll use the basic `DistilBertModel` and build our sentiment classifier on top of it.\n",
    "\n",
    "1. Load the base model (cased).\n",
    "2. run it on the sample text from previous section.\n",
    "4. Show pre-trained BERT's ability to classify our sample text.\n",
    "3. Build a Sentiment Analysis wrapper around BERT using Pytorch.\n",
    "5. Decide what loss function, optimizer, scheduler to use as well as the rest of the hyper-parameters.\n",
    "6. Create our training epoch function.\n",
    "7. Create our inference function.\n",
    "8. Finish with our training loop.\n",
    "9. View progress during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0P41FayISNRI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-cased', return_dict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFE7YSbFdY4t"
   },
   "source": [
    "And try to use it on the encoding of our sample text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape **without** using `.flatten()`: `(batch_size, 1, sequence_length)` = `(16, 1, 512)`.\n",
    "\n",
    "Shape **with** using `.flatten()`: `(batch_size, sequence_length)` = `(16, 512)`.\n",
    "\n",
    "We can use `.flatten()` within `DataLoader`, presumably as it already has information about the batch size and preprocesses accordingly.\n",
    "\n",
    "We shouldn't use `.flatten()` for casual inference, as the model expects an input of a fixed length and shape (?) (Here: 512, since this is a BERT-model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s1aoFxbQSn15"
   },
   "outputs": [],
   "source": [
    "sample_input_ids = torch.reshape(batch[\"input_ids\"][0], (1, 512))\n",
    "sample_attention_mask = torch.reshape(batch[\"attention_mask\"][0], (1, 512))\n",
    "\n",
    "sample = {'input_ids':sample_input_ids, 'attention_mask': sample_attention_mask}\n",
    "raw, pooled = bert_model(**sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4075,  0.2071,  0.1190,  ..., -0.4108,  0.0408, -0.0849],\n",
       "         [ 0.2758, -0.6349,  0.2261,  ..., -0.2547,  0.1820,  0.0482],\n",
       "         [ 0.5432, -0.6583,  0.1456,  ...,  0.6100,  0.6814, -0.2100],\n",
       "         ...,\n",
       "         [ 0.1340,  0.2772, -0.2694,  ...,  0.1435,  0.1378,  0.2156],\n",
       "         [ 0.1539,  0.2833, -0.3036,  ...,  0.1528,  0.1631,  0.2584],\n",
       "         [ 0.0829,  0.2475, -0.2725,  ...,  0.1568,  0.2055,  0.2240]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mLLu8zmqbaHV"
   },
   "source": [
    "The `raw` is a sequence of hidden states of the last layer of the model. Obtaining the pooled output `pooled` is done by applying the [BertPooler](https://github.com/huggingface/transformers/blob/edf0582c0be87b60f94f41c659ea779876efc7be/src/transformers/modeling_bert.py#L426) on the last hidden state `raw`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wTKi8-rTd_j4"
   },
   "source": [
    "You can think of `pooled` as a summary of the content, according to BERT. Albeit, you might try and do better. Let's look at the shape of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512, 768]), torch.Size([1, 768]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.shape, pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_mRflxPl32F"
   },
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased', return_dict=False)\n",
    "        self.drop = nn.Dropout(p=0.25)\n",
    "        self.output_layer = nn.Linear(768, num_classes)\n",
    "        # self.softmax = nn.Softmax(dim=1) # Why 1? to apply it among classes and not batches. Tell me more.\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        raw, pooled = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = self.drop(pooled)\n",
    "        fine_tuned = self.output_layer(pooled)\n",
    "        # classified = self.softmax(fine_tuned)\n",
    "        \n",
    "        return fine_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(df.Sentiment.unique())\n",
    "\n",
    "classifier = SentimentClassifier(num_classes)\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UJg8m3NQJahc"
   },
   "source": [
    "Our classifier delegates most of the heavy lifting to the DeBERTa model. We use a dropout layer for some regularization and a fully-connected layer for our output. **Note that we're returning the raw output of the last layer since that is required for the cross-entropy loss function in PyTorch to work.**\n",
    "\n",
    "This should work like any other PyTorch model. Let's create an instance and move it to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i0yQnuSFsjDp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEWARE  I recieved my box of 25 assorted Nonnie's Biscotti from Amazon . Every single biscotti was broken in 1-3 pieces. I am disgusted completely as these were to be favors of a wedding . I will never buy from this company again.  BEWARE\n",
      "tensor(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number2sentiment = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "sample = {'input_ids':sample_input_ids.to(device), 'attention_mask': sample_attention_mask.to(device)}\n",
    "\n",
    "pooled = classifier(**sample)\n",
    "probs = F.softmax(pooled, dim=1)\n",
    "\n",
    "print(batch['review'][0])\n",
    "print(batch['sentiment'][0])\n",
    "\n",
    "result = number2sentiment[torch.argmax(probs).item()]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.detach_()\n",
    "# del output\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g9xikRdtRN1N"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "76g7FV85H-T8"
   },
   "source": [
    "We need to define:\n",
    "\n",
    "1. Epochs.\n",
    "2. Learning rate\n",
    "3. Optimizer.\n",
    "4. Scheduler.\n",
    "5. Loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5v-ArJ2fCCcU"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "lr = 5e-5\n",
    "\n",
    "num_steps = EPOCHS * len(train_dl)\n",
    "\n",
    "optimizer = optim.AdamW(classifier.parameters(), lr=lr)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer=optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=num_steps)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A8522g7JIu5J"
   },
   "source": [
    "How come those?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bzl9UhuNx1_Q"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, loss_fn, optimizer, scheduler, device):\n",
    "    # 1. Set the model in training mode\n",
    "    model = model.train()\n",
    "    \n",
    "    # 2. Initialize the variables for tracking loss and correct predictions\n",
    "    loss_history = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    # 3. Iterate over the batches in the data loader\n",
    "    for batch in tqdm(dataloader):\n",
    "        \n",
    "        # a. Move the input and target tensors to the specified device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        targets = batch['sentiment'].to(device)\n",
    "        \n",
    "        # b. Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # only returns the processed 'pooled', as we defined in our S.C. class\n",
    "        \n",
    "        values, predictions = torch.max(outputs, dim=1) \n",
    "        # specifying the dim. changes the returns of torch.max()\n",
    "        \n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        # c. Update the loss and correct predictions variables\n",
    "        loss_history.append(loss.item())\n",
    "        correct_predictions += torch.sum(predictions == targets)\n",
    "        # torch compares values for numbers, regardless of dtype\n",
    "        \n",
    "        # d. Backward pass\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # 4. Compute the accuracy and average loss\n",
    "    accuracy = correct_predictions.double()/(len(dataloader) * 16)\n",
    "    average_loss = np.mean(loss_history)\n",
    "        \n",
    "    return accuracy, average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E4PniYIte0fr"
   },
   "source": [
    "Training the model should look familiar, except for two things. The scheduler gets called every time a batch is fed to the model. We're avoiding exploding gradients by clipping the gradients of the model using [clip_grad_norm_](https://pytorch.org/docs/stable/nn.html#clip-grad-norm).\n",
    "\n",
    "Let's write another one that helps us evaluate the model on a given data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXeRorVGIKre"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, loss_fn, device):\n",
    "    # 1. Set the model in evaluation mode\n",
    "    model = model.eval()\n",
    "    \n",
    "    # 2. Initialize the variables for tracking loss and correct predictions\n",
    "    loss_history = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 3. Iterate over the batches in the data loader\n",
    "        for batch in tqdm(dataloader):\n",
    "\n",
    "            # a. Move the input and target tensors to the specified device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            targets = batch['sentiment'].to(device)\n",
    "\n",
    "            # b. Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)        \n",
    "            values, predictions = torch.max(outputs, dim=1)        \n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # c. Update the loss and correct predictions variables\n",
    "            loss_history.append(loss.item())\n",
    "            correct_predictions += torch.sum(predictions == targets)\n",
    "\n",
    "        # 4. Compute the accuracy and average loss\n",
    "        accuracy = correct_predictions.double()/(len(dataloader) * 16)\n",
    "        average_loss = np.mean(loss_history)\n",
    "\n",
    "    return accuracy, average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_rdSDBHhhCh"
   },
   "source": [
    "Using those two, we can write our training loop. We'll also store the training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "1zhHoFNsxufs",
    "outputId": "2f11710a-700e-4933-b57e-5d50e5ed1f78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 3.\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4184/4184 [39:46<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training dataset: 0.7467286089866156\n",
      "Loss on training dataset: 0.5931135959189606\n",
      "-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 698/698 [02:30<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation dataset: 0.7821454154727794\n",
      "Loss on validation dataset: 0.53433882863525\n",
      "-----\n",
      "Epoch 2 out of 3.\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4184/4184 [39:54<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training dataset: 0.833144120458891\n",
      "Loss on training dataset: 0.4173259054600879\n",
      "-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 698/698 [02:30<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation dataset: 0.7933381088825214\n",
      "Loss on validation dataset: 0.4978796033653028\n",
      "-----\n",
      "Epoch 3 out of 3.\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4184/4184 [39:53<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training dataset: 0.9087595602294455\n",
      "Loss on training dataset: 0.25616456334308646\n",
      "-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 698/698 [02:30<00:00,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation dataset: 0.7905623209169054\n",
      "Loss on validation dataset: 0.6446215840600922\n",
      "-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.a) initialize a dict. with default values that are lists\n",
    "history = defaultdict(list)\n",
    "\n",
    "# 1.b) store the highest val. acc. seen so far\n",
    "best_accuracy = 0\n",
    "\n",
    "# 2. loop through epochs\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    # a. Print the current epoch number\n",
    "    print(f\"Epoch {epoch + 1} out of {EPOCHS}.\")\n",
    "    print('-' * 10)\n",
    "\n",
    "    # b. Train the model and print output\n",
    "    train_acc, train_loss = train_one_epoch(classifier, train_dl, loss_fn, optimizer, scheduler, device)\n",
    "    print(f\"Accuracy on training dataset: {train_acc}\")\n",
    "    print(f\"Loss on training dataset: {train_loss}\")\n",
    "    print('-' * 5)\n",
    "\n",
    "    # c. Evaluate on the validation dataset and print output\n",
    "    val_acc, val_loss = evaluate_model(classifier, val_dl, loss_fn, device)\n",
    "    print(f\"Accuracy on validation dataset: {val_acc}\")\n",
    "    print(f\"Loss on validation dataset: {val_loss}\")\n",
    "    print('-' * 5)\n",
    "\n",
    "    # d. Document the training and validation accuracy and loss in history\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    # If current val_acc > best_accuracy, model state is saved to file\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(classifier.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3HZb3NWFtFf"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Here, we will:\n",
    "1. Write a function to test how well the model generalizes, by using the test dataset, which is data it didn't train on.\n",
    "2. Evaluate the model's performance using classification report.\n",
    "3. Try it on raw input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgR6MuNS8jr_"
   },
   "outputs": [],
   "source": [
    "def get_predictions_from_dl(model, dataloader):\n",
    "    \n",
    "    # 1. Set the model to evaluation mode\n",
    "    model = model.eval()\n",
    "    \n",
    "    # 2. Initialize the lists for ins, true_outs, preds and confidence \n",
    "    reviews = []\n",
    "    predictions = []\n",
    "    confidence_levels = []\n",
    "    true_sentiments = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 3. Iterate over the batches in the data loader\n",
    "        for batch in tqdm(dataloader):\n",
    "            # a. Move the model input tensors to the specified device,\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # keeping the ins & outs of the dataset unmoved\n",
    "            texts = batch['review']\n",
    "            # using a different variable name to not overwrite the list\n",
    "            sentiments = batch['sentiment']\n",
    "\n",
    "            # b. Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            vals, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            # c. Get confidence levels with softmax \n",
    "            confidence_lvls = F.softmax(outputs, dim=1)\n",
    "\n",
    "            # d. Update the lists\n",
    "            reviews.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            confidence_levels.extend(confidence_lvls)\n",
    "            true_sentiments.extend(sentiments)\n",
    "\n",
    "    # 4. Stack, move back to cpu and return everything\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    confidence_levels = torch.stack(confidence_levels).cpu()\n",
    "    true_sentiments = torch.stack(true_sentiments).cpu()\n",
    "    \n",
    "    return reviews, predictions, confidence_levels, true_sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkbnBTI7kd_y"
   },
   "source": [
    "This is similar to the evaluation function, except ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 698/698 [02:19<00:00,  4.99it/s]\n"
     ]
    }
   ],
   "source": [
    "test_reviews, test_predictions, test_confidence_lvls, test_sentiments = get_predictions_from_dl(classifier, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVwoVij2lC7F"
   },
   "source": [
    "Let's have a look at the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "L8a9_8-ND3Is",
    "outputId": "9b2c48cc-b62e-41f3-dba5-af90457a37de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.79      0.78      0.78      3706\\n           1       0.70      0.72      0.71      3785\\n           2       0.89      0.88      0.88      3666\\n\\n    accuracy                           0.79     11157\\n   macro avg       0.79      0.79      0.79     11157\\nweighted avg       0.79      0.79      0.79     11157\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(test_sentiments, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iANBiY3sLo-K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The price is good, size is perfect...but my dog doesn't like them at all. Would not buy again sorry. Not the products fault\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'negative': tensor(0.7084),\n",
       " 'neutral': tensor(0.2869),\n",
       " 'positive': tensor(0.0047)}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 57\n",
    "\n",
    "review = test_reviews[idx]\n",
    "confidence_lvls = test_confidence_lvls[idx]\n",
    "\n",
    "print(review)\n",
    "conf_lvls = {number2sentiment[idx] : confidence_lvls[idx] for idx in range(3)}\n",
    "conf_lvls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WL5pDmvFyaU"
   },
   "source": [
    "### Predicting on Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = r\"\"\"Very easy game. Explained in a minute. Anyone can play it. Very fast laps. The limit could be another two or three centimeters shorter,\n",
    "then it will “click” faster! I did it this way. It can be packed up to a small size and is almost indestructible.\n",
    "But it can easily lead to frustration if you play with too much ambition. You also have to be able to lose!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode_plus(Text, add_special_tokens=True, max_length=512,\n",
    "                                 padding='max_length', truncation=True,\n",
    "                                 return_token_type_ids=False, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': 0.001904321019537747, 'neutral': 0.023873839527368546, 'positive': 0.97422194480896}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "pooled = classifier(input_ids, attention_mask)\n",
    "\n",
    "confidence = F.softmax(pooled, dim=1).tolist()[0]\n",
    "confidence = {number2sentiment[idx] : confidence[idx] for idx in range(3)}\n",
    "\n",
    "val, prediction = torch.max(pooled, dim=1)\n",
    "prediction = number2sentiment[prediction.item()]\n",
    "\n",
    "print(confidence)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PVhwzq7bpPRl"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wf39tauBa2V2"
   },
   "source": [
    "## References\n",
    "\n",
    "- [Huggingface Transformers](https://huggingface.co/transformers/)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "08.sentiment-analysis-with-bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
